{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO DE APRENDIZAJE ESTADISTICO\n",
    "\n",
    "## Relacion entre el uso de redes sociales y la productividad academica en estudiantes universitarios peruanos\n",
    "\n",
    "---\n",
    "\n",
    "**Universidad:** Universidad Privada Antenor Orrego  \n",
    "**Curso:** Aprendizaje Estadistico  \n",
    "**Semestre:** 2025-20  \n",
    "**Docente:** Hernan Sagastegui Chigne\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. INTRODUCCION\n",
    "\n",
    "### 1.1 Contexto del Estudio\n",
    "\n",
    "Este proyecto analiza la relacion entre el uso de redes sociales y el rendimiento academico en estudiantes universitarios peruanos. Segun estudios recientes:\n",
    "\n",
    "- Estudio UNAP 2024: 51% de estudiantes presenta uso moderado de RRSS, 44% uso bajo, 5% uso elevado\n",
    "- Universidad Peruana Los Andes 2023: 49.3% de estudiantes con bajo rendimiento y 57% con uso intermedio de RRSS\n",
    "- Facultad de Medicina Lima 2019: 50% de estudiantes usan RRSS 1 hora o mas diariamente\n",
    "\n",
    "### 1.2 Objetivo\n",
    "\n",
    "Desarrollar un modelo de Machine Learning capaz de predecir el rendimiento academico de estudiantes universitarios basandose en sus habitos de uso de redes sociales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. CONFIGURACION DEL ENTORNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 1: INSTALACION E IMPORTACION DE LIBRERIAS\n",
    "# ============================================================================\n",
    "\n",
    "# Instalar librerias necesarias (ejecutar solo si es necesario)\n",
    "# !pip install pandas numpy scikit-learn matplotlib seaborn openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             accuracy_score, ConfusionMatrixDisplay)\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuracion de visualizacion\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROYECTO: RELACION ENTRE USO DE REDES SOCIALES Y RENDIMIENTO ACADEMICO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nLibrerias importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. GENERACION DEL DATASET\n",
    "\n",
    "Dataset sintetico basado en estudios reales de universidades peruanas con 600 estudiantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 2: GENERACION DEL DATASET\n",
    "# ============================================================================\n",
    "# Dataset basado en estudios reales de universidades peruanas:\n",
    "# - Estudio UNAP 2024: 51% uso moderado, 44% uso bajo, 5% uso alto\n",
    "# - Estudio Universidad Peruana Los Andes 2023: 49.3% bajo rendimiento\n",
    "# - Estudio Lima 2019: 50% usan RRSS 1 hora o mas diariamente\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] GENERACION DEL DATASET\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "np.random.seed(2025)\n",
    "n_samples = 600\n",
    "\n",
    "# --- DATOS DEMOGRAFICOS PERUANOS ---\n",
    "\n",
    "# Carreras comunes en universidades peruanas\n",
    "carreras_peru = [\n",
    "    'Ing. de Sistemas', 'Ing. Industrial', 'Medicina', 'Derecho', \n",
    "    'Contabilidad', 'Psicologia', 'Administracion', 'Arquitectura',\n",
    "    'Ing. Civil', 'Enfermeria', 'Educacion', 'Economia',\n",
    "    'Comunicaciones', 'Marketing', 'Ing. Ambiental', 'Odontologia'\n",
    "]\n",
    "\n",
    "# Ciudades peruanas principales\n",
    "ciudades_peru = [\n",
    "    'Lima', 'Trujillo', 'Arequipa', 'Chiclayo', 'Piura', \n",
    "    'Cusco', 'Huancayo', 'Puno', 'Ica', 'Tacna'\n",
    "]\n",
    "\n",
    "# Generar datos demograficos\n",
    "edades = np.random.choice(range(17, 30), size=n_samples, \n",
    "                          p=[0.04, 0.12, 0.16, 0.15, 0.14, 0.12, 0.10, \n",
    "                             0.07, 0.05, 0.03, 0.01, 0.008, 0.002])\n",
    "generos = np.random.choice(['Masculino', 'Femenino'], size=n_samples, p=[0.46, 0.54])\n",
    "carreras = np.random.choice(carreras_peru, size=n_samples)\n",
    "ciclos = np.random.choice(range(1, 11), size=n_samples, \n",
    "                          p=[0.08, 0.10, 0.12, 0.12, 0.12, 0.12, 0.10, 0.10, 0.08, 0.06])\n",
    "ciudades = np.random.choice(ciudades_peru, size=n_samples,\n",
    "                            p=[0.35, 0.12, 0.10, 0.08, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04])\n",
    "\n",
    "# --- USO DE REDES SOCIALES ---\n",
    "# Media de 4.5 horas con desviacion de 2.3 (basado en literatura peruana)\n",
    "horas_redes_sociales = np.clip(np.random.normal(4.5, 2.3, n_samples), 0.5, 12).round(1)\n",
    "\n",
    "# Redes sociales mas usadas en Peru (TikTok y WhatsApp dominan segun estudios 2024)\n",
    "redes_sociales = np.random.choice([\n",
    "    'TikTok', 'WhatsApp', 'Instagram', 'Facebook', 'YouTube', \n",
    "    'Twitter', 'Discord', 'LinkedIn', 'Telegram'\n",
    "], size=n_samples, p=[0.28, 0.24, 0.18, 0.12, 0.08, 0.04, 0.03, 0.02, 0.01])\n",
    "\n",
    "# Motivo de uso\n",
    "motivos_uso = np.random.choice([\n",
    "    'Entretenimiento', 'Socializacion', 'Academico', 'Noticias', 'Trabajo'\n",
    "], size=n_samples, p=[0.38, 0.28, 0.18, 0.10, 0.06])\n",
    "\n",
    "# Frecuencia de afectacion a la concentracion\n",
    "afecta_concentracion = np.random.choice([\n",
    "    'Siempre', 'Frecuentemente', 'A veces', 'Rara vez', 'Nunca'\n",
    "], size=n_samples, p=[0.18, 0.30, 0.30, 0.15, 0.07])\n",
    "\n",
    "# --- HABITOS DE ESTUDIO ---\n",
    "# Horas de estudio (correlacion inversa con uso de RRSS)\n",
    "horas_estudio_base = 6.0 - (horas_redes_sociales * 0.55) + np.random.normal(0, 1.3, n_samples)\n",
    "for i in range(n_samples):\n",
    "    if motivos_uso[i] == 'Academico':\n",
    "        horas_estudio_base[i] += np.random.uniform(1.0, 2.2)\n",
    "    if motivos_uso[i] == 'Trabajo':\n",
    "        horas_estudio_base[i] -= np.random.uniform(0.3, 0.8)\n",
    "horas_estudio = np.clip(horas_estudio_base, 0.5, 10).round(1)\n",
    "\n",
    "# Percepcion de afectacion a horas de estudio\n",
    "afecta_horas_estudio = []\n",
    "for i in range(n_samples):\n",
    "    if horas_redes_sociales[i] > 6:\n",
    "        afecta_horas_estudio.append(np.random.choice(['Si', 'No'], p=[0.82, 0.18]))\n",
    "    elif horas_redes_sociales[i] > 4:\n",
    "        afecta_horas_estudio.append(np.random.choice(['Si', 'No'], p=[0.58, 0.42]))\n",
    "    else:\n",
    "        afecta_horas_estudio.append(np.random.choice(['Si', 'No'], p=[0.28, 0.72]))\n",
    "\n",
    "# Uso de estrategias para evitar distracciones\n",
    "usa_estrategias = np.random.choice(['Si', 'No'], size=n_samples, p=[0.38, 0.62])\n",
    "\n",
    "# Percepcion del impacto general\n",
    "impacto_general = []\n",
    "for i in range(n_samples):\n",
    "    if horas_redes_sociales[i] > 6:\n",
    "        impacto_general.append(np.random.choice(\n",
    "            ['Muy Negativo', 'Negativo', 'Neutral', 'Positivo', 'Muy Positivo'], \n",
    "            p=[0.22, 0.38, 0.25, 0.12, 0.03]))\n",
    "    elif motivos_uso[i] == 'Academico':\n",
    "        impacto_general.append(np.random.choice(\n",
    "            ['Muy Negativo', 'Negativo', 'Neutral', 'Positivo', 'Muy Positivo'], \n",
    "            p=[0.03, 0.08, 0.25, 0.44, 0.20]))\n",
    "    else:\n",
    "        impacto_general.append(np.random.choice(\n",
    "            ['Muy Negativo', 'Negativo', 'Neutral', 'Positivo', 'Muy Positivo'], \n",
    "            p=[0.10, 0.24, 0.36, 0.22, 0.08]))\n",
    "\n",
    "# --- RENDIMIENTO ACADEMICO ---\n",
    "# Promedio ponderado (escala 0-20, sistema peruano)\n",
    "promedio_base = 12.5 - (horas_redes_sociales * 0.55) + (horas_estudio * 0.85) + np.random.normal(0, 2.0, n_samples)\n",
    "for i in range(n_samples):\n",
    "    if motivos_uso[i] == 'Academico':\n",
    "        promedio_base[i] += np.random.uniform(0.5, 1.5)\n",
    "    if usa_estrategias[i] == 'Si':\n",
    "        promedio_base[i] += np.random.uniform(0.3, 1.0)\n",
    "    if afecta_concentracion[i] == 'Siempre':\n",
    "        promedio_base[i] -= np.random.uniform(1.0, 2.0)\n",
    "    elif afecta_concentracion[i] == 'Frecuentemente':\n",
    "        promedio_base[i] -= np.random.uniform(0.5, 1.0)\n",
    "promedio_ponderado = np.clip(promedio_base, 5, 20).round(2)\n",
    "\n",
    "# Variable objetivo categorica\n",
    "# Bajo: < 11, Promedio: 11-14.5, Alto: > 14.5\n",
    "rendimiento_academico = []\n",
    "for prom in promedio_ponderado:\n",
    "    if prom < 11.0:\n",
    "        rendimiento_academico.append('Bajo')\n",
    "    elif prom < 14.5:\n",
    "        rendimiento_academico.append('Promedio')\n",
    "    else:\n",
    "        rendimiento_academico.append('Alto')\n",
    "\n",
    "# Crear DataFrame inicial\n",
    "data_raw = pd.DataFrame({\n",
    "    'ID': range(1, n_samples + 1),\n",
    "    'Edad': edades,\n",
    "    'Genero': generos,\n",
    "    'Ciudad': ciudades,\n",
    "    'Carrera': carreras,\n",
    "    'Ciclo': ciclos,\n",
    "    'Horas_Redes_Sociales': horas_redes_sociales,\n",
    "    'Red_Social_Principal': redes_sociales,\n",
    "    'Motivo_Uso': motivos_uso,\n",
    "    'Afecta_Concentracion': afecta_concentracion,\n",
    "    'Horas_Estudio': horas_estudio,\n",
    "    'Afecta_Horas_Estudio': afecta_horas_estudio,\n",
    "    'Usa_Estrategias': usa_estrategias,\n",
    "    'Impacto_General': impacto_general,\n",
    "    'Promedio_Ponderado': promedio_ponderado,\n",
    "    'Rendimiento_Academico': rendimiento_academico\n",
    "})\n",
    "\n",
    "print(\"Dataset inicial generado: {} registros\".format(len(data_raw)))\n",
    "print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. LIMPIEZA Y PREPROCESAMIENTO DE DATOS\n",
    "\n",
    "Proceso riguroso de limpieza siguiendo las mejores practicas de Data Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 3: LIMPIEZA Y PREPROCESAMIENTO DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2] LIMPIEZA Y PREPROCESAMIENTO DE DATOS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Crear copia para limpieza\n",
    "data = data_raw.copy()\n",
    "\n",
    "# 3.1 Verificar valores nulos\n",
    "print(\"\\n3.1 VERIFICACION DE VALORES NULOS:\")\n",
    "nulos = data.isnull().sum()\n",
    "total_nulos = nulos.sum()\n",
    "print(\"    Total de valores nulos encontrados: {}\".format(total_nulos))\n",
    "if total_nulos > 0:\n",
    "    print(\"    Columnas con valores nulos:\")\n",
    "    print(nulos[nulos > 0])\n",
    "    # Eliminar filas con valores nulos\n",
    "    data = data.dropna()\n",
    "    print(\"    Filas con nulos eliminadas\")\n",
    "else:\n",
    "    print(\"    [OK] No se encontraron valores nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Verificar y eliminar duplicados\n",
    "print(\"\\n3.2 VERIFICACION DE DUPLICADOS:\")\n",
    "duplicados = data.duplicated().sum()\n",
    "print(\"    Registros duplicados encontrados: {}\".format(duplicados))\n",
    "if duplicados > 0:\n",
    "    data = data.drop_duplicates()\n",
    "    print(\"    Duplicados eliminados. Nuevos registros: {}\".format(len(data)))\n",
    "else:\n",
    "    print(\"    [OK] No se encontraron duplicados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Verificar tipos de datos\n",
    "print(\"\\n3.3 VERIFICACION DE TIPOS DE DATOS:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Verificar rangos de valores numericos\n",
    "print(\"\\n3.4 VERIFICACION DE RANGOS VALIDOS:\")\n",
    "\n",
    "registros_inicial = len(data)\n",
    "\n",
    "# Edad: debe estar entre 16 y 35 (rango universitario)\n",
    "edad_invalida = len(data[(data['Edad'] < 16) | (data['Edad'] > 35)])\n",
    "print(\"    Edades fuera de rango [16-35]: {}\".format(edad_invalida))\n",
    "data = data[(data['Edad'] >= 16) & (data['Edad'] <= 35)]\n",
    "\n",
    "# Ciclo: debe estar entre 1 y 10\n",
    "ciclo_invalido = len(data[(data['Ciclo'] < 1) | (data['Ciclo'] > 10)])\n",
    "print(\"    Ciclos fuera de rango [1-10]: {}\".format(ciclo_invalido))\n",
    "data = data[(data['Ciclo'] >= 1) & (data['Ciclo'] <= 10)]\n",
    "\n",
    "# Horas de redes sociales: debe estar entre 0 y 15\n",
    "horas_rrss_invalida = len(data[(data['Horas_Redes_Sociales'] < 0) | (data['Horas_Redes_Sociales'] > 15)])\n",
    "print(\"    Horas RRSS fuera de rango [0-15]: {}\".format(horas_rrss_invalida))\n",
    "data = data[(data['Horas_Redes_Sociales'] >= 0) & (data['Horas_Redes_Sociales'] <= 15)]\n",
    "\n",
    "# Horas de estudio: debe estar entre 0 y 12\n",
    "horas_estudio_invalida = len(data[(data['Horas_Estudio'] < 0) | (data['Horas_Estudio'] > 12)])\n",
    "print(\"    Horas estudio fuera de rango [0-12]: {}\".format(horas_estudio_invalida))\n",
    "data = data[(data['Horas_Estudio'] >= 0) & (data['Horas_Estudio'] <= 12)]\n",
    "\n",
    "# Promedio ponderado: debe estar entre 0 y 20 (sistema peruano)\n",
    "promedio_invalido = len(data[(data['Promedio_Ponderado'] < 0) | (data['Promedio_Ponderado'] > 20)])\n",
    "print(\"    Promedios fuera de rango [0-20]: {}\".format(promedio_invalido))\n",
    "data = data[(data['Promedio_Ponderado'] >= 0) & (data['Promedio_Ponderado'] <= 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Verificar valores categoricos validos\n",
    "print(\"\\n3.5 VERIFICACION DE VALORES CATEGORICOS:\")\n",
    "\n",
    "# Genero\n",
    "generos_validos = ['Masculino', 'Femenino']\n",
    "generos_invalidos = len(data[~data['Genero'].isin(generos_validos)])\n",
    "print(\"    Generos invalidos: {}\".format(generos_invalidos))\n",
    "data = data[data['Genero'].isin(generos_validos)]\n",
    "\n",
    "# Motivo de uso\n",
    "motivos_validos = ['Entretenimiento', 'Socializacion', 'Academico', 'Noticias', 'Trabajo']\n",
    "motivos_invalidos = len(data[~data['Motivo_Uso'].isin(motivos_validos)])\n",
    "print(\"    Motivos de uso invalidos: {}\".format(motivos_invalidos))\n",
    "data = data[data['Motivo_Uso'].isin(motivos_validos)]\n",
    "\n",
    "# Afecta concentracion\n",
    "concentracion_valida = ['Siempre', 'Frecuentemente', 'A veces', 'Rara vez', 'Nunca']\n",
    "concentracion_invalidos = len(data[~data['Afecta_Concentracion'].isin(concentracion_valida)])\n",
    "print(\"    Valores de concentracion invalidos: {}\".format(concentracion_invalidos))\n",
    "data = data[data['Afecta_Concentracion'].isin(concentracion_valida)]\n",
    "\n",
    "# Variables binarias\n",
    "binarios_validos = ['Si', 'No']\n",
    "data = data[data['Afecta_Horas_Estudio'].isin(binarios_validos)]\n",
    "data = data[data['Usa_Estrategias'].isin(binarios_validos)]\n",
    "\n",
    "# Impacto general\n",
    "impacto_valido = ['Muy Negativo', 'Negativo', 'Neutral', 'Positivo', 'Muy Positivo']\n",
    "data = data[data['Impacto_General'].isin(impacto_valido)]\n",
    "\n",
    "# Rendimiento academico\n",
    "rendimiento_valido = ['Bajo', 'Promedio', 'Alto']\n",
    "data = data[data['Rendimiento_Academico'].isin(rendimiento_valido)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Detectar y tratar outliers usando IQR\n",
    "print(\"\\n3.6 DETECCION DE OUTLIERS (Metodo IQR):\")\n",
    "\n",
    "def detectar_outliers_iqr(df, columna):\n",
    "    Q1 = df[columna].quantile(0.25)\n",
    "    Q3 = df[columna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[columna] < limite_inferior) | (df[columna] > limite_superior)]\n",
    "    return len(outliers), limite_inferior, limite_superior\n",
    "\n",
    "columnas_numericas = ['Edad', 'Ciclo', 'Horas_Redes_Sociales', 'Horas_Estudio', 'Promedio_Ponderado']\n",
    "\n",
    "for col in columnas_numericas:\n",
    "    n_outliers, lim_inf, lim_sup = detectar_outliers_iqr(data, col)\n",
    "    print(\"    {} - Outliers: {} (Limites: [{:.2f}, {:.2f}])\".format(\n",
    "        col, n_outliers, lim_inf, lim_sup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 Reiniciar indices y verificar resultado final\n",
    "data = data.reset_index(drop=True)\n",
    "data['ID'] = range(1, len(data) + 1)\n",
    "\n",
    "print(\"\\n3.7 RESUMEN DE LIMPIEZA:\")\n",
    "print(\"    Registros originales: {}\".format(len(data_raw)))\n",
    "print(\"    Registros despues de limpieza: {}\".format(len(data)))\n",
    "print(\"    Registros eliminados: {}\".format(len(data_raw) - len(data)))\n",
    "print(\"    Porcentaje de datos conservados: {:.2f}%\".format(len(data)/len(data_raw)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.8 Guardar dataset limpio\n",
    "data.to_excel('dataset_rrss_rendimiento_limpio.xlsx', index=False)\n",
    "print(\"\\n    Dataset limpio guardado: dataset_rrss_rendimiento_limpio.xlsx\")\n",
    "\n",
    "# Mostrar informacion del dataset limpio\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFORMACION DEL DATASET LIMPIO\")\n",
    "print(\"=\"*70)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. ANALISIS EXPLORATORIO DE DATOS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 4: ANALISIS EXPLORATORIO DE DATOS (EDA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3] ANALISIS EXPLORATORIO DE DATOS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# 4.1 Estadisticas descriptivas\n",
    "print(\"\\n4.1 ESTADISTICAS DESCRIPTIVAS DE VARIABLES NUMERICAS:\")\n",
    "estadisticas = data[['Edad', 'Ciclo', 'Horas_Redes_Sociales', 'Horas_Estudio', 'Promedio_Ponderado']].describe()\n",
    "print(estadisticas.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Distribucion de la variable objetivo\n",
    "print(\"\\n4.2 DISTRIBUCION DE LA VARIABLE OBJETIVO (Rendimiento_Academico):\")\n",
    "distribucion = data['Rendimiento_Academico'].value_counts()\n",
    "print(distribucion)\n",
    "print(\"\\nPorcentajes:\")\n",
    "print((distribucion / len(data) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Matriz de correlacion\n",
    "print(\"\\n4.3 MATRIZ DE CORRELACION (variables numericas):\")\n",
    "numeric_cols = ['Edad', 'Ciclo', 'Horas_Redes_Sociales', 'Horas_Estudio', 'Promedio_Ponderado']\n",
    "corr_matrix = data[numeric_cols].corr()\n",
    "print(corr_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Correlaciones clave\n",
    "print(\"\\n4.4 CORRELACIONES CLAVE:\")\n",
    "corr_rrss_prom = data['Horas_Redes_Sociales'].corr(data['Promedio_Ponderado'])\n",
    "corr_est_prom = data['Horas_Estudio'].corr(data['Promedio_Ponderado'])\n",
    "corr_rrss_est = data['Horas_Redes_Sociales'].corr(data['Horas_Estudio'])\n",
    "\n",
    "print(\"    Horas_Redes_Sociales vs Promedio_Ponderado: {:.3f}\".format(corr_rrss_prom))\n",
    "print(\"    Horas_Estudio vs Promedio_Ponderado: {:.3f}\".format(corr_est_prom))\n",
    "print(\"    Horas_Redes_Sociales vs Horas_Estudio: {:.3f}\".format(corr_rrss_est))\n",
    "\n",
    "# Interpretacion\n",
    "print(\"\\n    INTERPRETACION:\")\n",
    "if corr_rrss_prom < -0.5:\n",
    "    print(\"    - Existe una correlacion NEGATIVA MODERADA-FUERTE entre uso de RRSS y rendimiento\")\n",
    "if corr_est_prom > 0.5:\n",
    "    print(\"    - Existe una correlacion POSITIVA MODERADA-FUERTE entre horas de estudio y rendimiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. VISUALIZACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 5: VISUALIZACIONES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4] GENERACION DE VISUALIZACIONES\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Grafico 1: Distribucion de variable objetivo y horas de RRSS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribucion de Rendimiento Academico\n",
    "colors_rend = {'Alto': '#2ecc71', 'Promedio': '#3498db', 'Bajo': '#e74c3c'}\n",
    "counts = data['Rendimiento_Academico'].value_counts()\n",
    "bars = axes[0].bar(counts.index, counts.values, \n",
    "                   color=[colors_rend[x] for x in counts.index], \n",
    "                   edgecolor='black')\n",
    "axes[0].set_xlabel('Rendimiento Academico', fontsize=12)\n",
    "axes[0].set_ylabel('Cantidad de Estudiantes', fontsize=12)\n",
    "axes[0].set_title('Distribucion de la Variable Objetivo\\n(n={})'.format(len(data)), fontsize=14)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                 '{}'.format(int(height)), ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Histograma de Horas en Redes Sociales\n",
    "axes[1].hist(data['Horas_Redes_Sociales'], bins=20, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Horas Diarias en Redes Sociales', fontsize=12)\n",
    "axes[1].set_ylabel('Frecuencia', fontsize=12)\n",
    "axes[1].set_title('Distribucion del Uso de Redes Sociales', fontsize=14)\n",
    "axes[1].axvline(data['Horas_Redes_Sociales'].mean(), color='red', linestyle='--', \n",
    "                label='Media: {:.1f}h'.format(data['Horas_Redes_Sociales'].mean()))\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grafico_1_distribucion.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"   Grafico 1 guardado: grafico_1_distribucion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico 2: Scatter plots de correlaciones\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors_map = {'Alto': '#2ecc71', 'Promedio': '#3498db', 'Bajo': '#e74c3c'}\n",
    "\n",
    "for clase in ['Alto', 'Promedio', 'Bajo']:\n",
    "    subset = data[data['Rendimiento_Academico'] == clase]\n",
    "    axes[0].scatter(subset['Horas_Redes_Sociales'], subset['Promedio_Ponderado'], \n",
    "                    label=clase, alpha=0.6, s=40, c=colors_map[clase])\n",
    "axes[0].set_xlabel('Horas Diarias en Redes Sociales', fontsize=12)\n",
    "axes[0].set_ylabel('Promedio Ponderado', fontsize=12)\n",
    "corr1 = data['Horas_Redes_Sociales'].corr(data['Promedio_Ponderado'])\n",
    "axes[0].set_title('Uso de RRSS vs Rendimiento Academico\\n(Correlacion: {:.2f})'.format(corr1), fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "for clase in ['Alto', 'Promedio', 'Bajo']:\n",
    "    subset = data[data['Rendimiento_Academico'] == clase]\n",
    "    axes[1].scatter(subset['Horas_Estudio'], subset['Promedio_Ponderado'], \n",
    "                    label=clase, alpha=0.6, s=40, c=colors_map[clase])\n",
    "axes[1].set_xlabel('Horas Diarias de Estudio', fontsize=12)\n",
    "axes[1].set_ylabel('Promedio Ponderado', fontsize=12)\n",
    "corr2 = data['Horas_Estudio'].corr(data['Promedio_Ponderado'])\n",
    "axes[1].set_title('Horas de Estudio vs Rendimiento Academico\\n(Correlacion: {:.2f})'.format(corr2), fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grafico_2_correlaciones.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"   Grafico 2 guardado: grafico_2_correlaciones.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico 3: Analisis por variables categoricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
    "\n",
    "# Por Red Social\n",
    "rs_rend = data.groupby(['Red_Social_Principal', 'Rendimiento_Academico']).size().unstack(fill_value=0)\n",
    "rs_rend.plot(kind='bar', ax=axes[0,0], color=colors)\n",
    "axes[0,0].set_xlabel('Red Social Principal', fontsize=11)\n",
    "axes[0,0].set_ylabel('Cantidad', fontsize=11)\n",
    "axes[0,0].set_title('Rendimiento segun Red Social Principal', fontsize=13)\n",
    "axes[0,0].legend(title='Rendimiento')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Por Motivo de Uso\n",
    "mu_rend = data.groupby(['Motivo_Uso', 'Rendimiento_Academico']).size().unstack(fill_value=0)\n",
    "mu_rend.plot(kind='bar', ax=axes[0,1], color=colors)\n",
    "axes[0,1].set_xlabel('Motivo de Uso', fontsize=11)\n",
    "axes[0,1].set_ylabel('Cantidad', fontsize=11)\n",
    "axes[0,1].set_title('Rendimiento segun Motivo de Uso', fontsize=13)\n",
    "axes[0,1].legend(title='Rendimiento')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Por Afecta Concentracion\n",
    "orden = ['Nunca', 'Rara vez', 'A veces', 'Frecuentemente', 'Siempre']\n",
    "ac_rend = data.groupby(['Afecta_Concentracion', 'Rendimiento_Academico']).size().unstack(fill_value=0)\n",
    "ac_rend = ac_rend.reindex(orden)\n",
    "ac_rend.plot(kind='bar', ax=axes[1,0], color=colors)\n",
    "axes[1,0].set_xlabel('Afecta Concentracion', fontsize=11)\n",
    "axes[1,0].set_ylabel('Cantidad', fontsize=11)\n",
    "axes[1,0].set_title('Rendimiento segun Efecto en Concentracion', fontsize=13)\n",
    "axes[1,0].legend(title='Rendimiento')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Boxplot de Horas RRSS por rendimiento\n",
    "orden_rend = ['Bajo', 'Promedio', 'Alto']\n",
    "data_box = data.copy()\n",
    "data_box['Rendimiento_Academico'] = pd.Categorical(\n",
    "    data_box['Rendimiento_Academico'], categories=orden_rend, ordered=True)\n",
    "data_box.boxplot(column='Horas_Redes_Sociales', by='Rendimiento_Academico', ax=axes[1,1])\n",
    "axes[1,1].set_xlabel('Rendimiento Academico', fontsize=11)\n",
    "axes[1,1].set_ylabel('Horas en Redes Sociales', fontsize=11)\n",
    "axes[1,1].set_title('Distribucion de Horas en RRSS por Rendimiento', fontsize=13)\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grafico_3_analisis_categorico.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"   Grafico 3 guardado: grafico_3_analisis_categorico.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico 4: Mapa de calor de correlaciones\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', square=True, ax=ax, linewidths=0.5)\n",
    "ax.set_title('Matriz de Correlacion de Variables Numericas', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('grafico_4_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"   Grafico 4 guardado: grafico_4_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. PREPARACION DE DATOS PARA EL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 6: PREPARACION DE DATOS PARA EL MODELO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5] PREPARACION DE DATOS PARA EL MODELO\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Definir caracteristicas (X) y objetivo (y)\n",
    "feature_columns = ['Edad', 'Ciclo', 'Horas_Redes_Sociales', 'Horas_Estudio', \n",
    "                   'Red_Social_Principal', 'Motivo_Uso', 'Afecta_Concentracion',\n",
    "                   'Afecta_Horas_Estudio', 'Usa_Estrategias', 'Impacto_General']\n",
    "\n",
    "X = data[feature_columns]\n",
    "y = data['Rendimiento_Academico']\n",
    "\n",
    "# Codificar variable objetivo\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"\\n5.1 Codificacion de la variable objetivo:\")\n",
    "for i, clase in enumerate(label_encoder.classes_):\n",
    "    print(\"    {} --> {}\".format(clase, i))\n",
    "\n",
    "# Identificar columnas numericas y categoricas\n",
    "numeric_features = ['Edad', 'Ciclo', 'Horas_Redes_Sociales', 'Horas_Estudio']\n",
    "categorical_features = ['Red_Social_Principal', 'Motivo_Uso', 'Afecta_Concentracion',\n",
    "                        'Afecta_Horas_Estudio', 'Usa_Estrategias', 'Impacto_General']\n",
    "\n",
    "print(\"\\n5.2 Caracteristicas identificadas:\")\n",
    "print(\"    Numericas ({}): {}\".format(len(numeric_features), numeric_features))\n",
    "print(\"    Categoricas ({}): {}\".format(len(categorical_features), categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 7: DIVISION DEL DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6] DIVISION DEL DATASET\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(\"\\n6.1 Tamanio de los conjuntos:\")\n",
    "print(\"    Entrenamiento: {} muestras (80%)\".format(len(X_train)))\n",
    "print(\"    Prueba: {} muestras (20%)\".format(len(X_test)))\n",
    "\n",
    "print(\"\\n6.2 Distribucion en conjunto de entrenamiento:\")\n",
    "for i, clase in enumerate(label_encoder.classes_):\n",
    "    count = sum(y_train == i)\n",
    "    print(\"    {}: {} ({:.1f}%)\".format(clase, count, count/len(y_train)*100))\n",
    "\n",
    "print(\"\\n6.3 Distribucion en conjunto de prueba:\")\n",
    "for i, clase in enumerate(label_encoder.classes_):\n",
    "    count = sum(y_test == i)\n",
    "    print(\"    {}: {} ({:.1f}%)\".format(clase, count, count/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. CONSTRUCCION Y ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 8: CONSTRUCCION DEL PIPELINE Y MODELO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[7] CONSTRUCCION DEL MODELO\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Crear preprocesador\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline con Arbol de Decision\n",
    "dt_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42, max_depth=6, min_samples_leaf=5))\n",
    "])\n",
    "\n",
    "# Pipeline con Random Forest\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_estimators=100, max_depth=8))\n",
    "])\n",
    "\n",
    "print(\"\\n7.1 Modelos configurados:\")\n",
    "print(\"    - Arbol de Decision (max_depth=6, min_samples_leaf=5)\")\n",
    "print(\"    - Random Forest (n_estimators=100, max_depth=8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 9: VALIDACION CRUZADA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[8] VALIDACION CRUZADA (K-Fold, k=10)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Arbol de Decision\n",
    "dt_cv_scores = cross_val_score(dt_pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"\\n8.1 Arbol de Decision:\")\n",
    "print(\"    Scores por fold: {}\".format(dt_cv_scores.round(3)))\n",
    "print(\"    Exactitud promedio: {:.3f} (+/- {:.3f})\".format(dt_cv_scores.mean(), dt_cv_scores.std() * 2))\n",
    "\n",
    "# Random Forest\n",
    "rf_cv_scores = cross_val_score(rf_pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"\\n8.2 Random Forest:\")\n",
    "print(\"    Scores por fold: {}\".format(rf_cv_scores.round(3)))\n",
    "print(\"    Exactitud promedio: {:.3f} (+/- {:.3f})\".format(rf_cv_scores.mean(), rf_cv_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 10: ENTRENAMIENTO FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[9] ENTRENAMIENTO DEL MODELO FINAL\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n    [OK] Arbol de Decision entrenado exitosamente\")\n",
    "print(\"    [OK] Random Forest entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 11: EVALUACION EN CONJUNTO DE PRUEBA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[10] EVALUACION EN CONJUNTO DE PRUEBA\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_dt = dt_pipeline.predict(X_test)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Arbol de Decision\n",
    "print(\"\\n10.1 ARBOL DE DECISION - Resultados en Test:\")\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"    Exactitud (Accuracy): {:.3f} ({:.1f}%)\".format(acc_dt, acc_dt*100))\n",
    "print(\"\\n    Reporte de Clasificacion:\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"\\n10.2 RANDOM FOREST - Resultados en Test:\")\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"    Exactitud (Accuracy): {:.3f} ({:.1f}%)\".format(acc_rf, acc_rf*100))\n",
    "print(\"\\n    Reporte de Clasificacion:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de Confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=label_encoder.classes_)\n",
    "disp_dt.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Matriz de Confusion - Arbol de Decision\\n(Accuracy: {:.1f}%)'.format(acc_dt*100), fontsize=12)\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=label_encoder.classes_)\n",
    "disp_rf.plot(ax=axes[1], cmap='Blues', values_format='d')\n",
    "axes[1].set_title('Matriz de Confusion - Random Forest\\n(Accuracy: {:.1f}%)'.format(acc_rf*100), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grafico_5_matrices_confusion.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n   Grafico 5 guardado: grafico_5_matrices_confusion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 12: IMPORTANCIA DE CARACTERISTICAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[11] IMPORTANCIA DE CARACTERISTICAS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Obtener nombres de caracteristicas\n",
    "feature_names = (numeric_features + \n",
    "                 list(rf_pipeline.named_steps['preprocessor']\n",
    "                      .named_transformers_['cat']\n",
    "                      .get_feature_names_out(categorical_features)))\n",
    "\n",
    "importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Caracteristica': feature_names,\n",
    "    'Importancia': importances\n",
    "}).sort_values('Importancia', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 caracteristicas mas importantes:\")\n",
    "for idx, row in feature_importance_df.head(15).iterrows():\n",
    "    print(\"    {}: {:.4f} ({:.2f}%)\".format(row['Caracteristica'], row['Importancia'], row['Importancia']*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico de importancia\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features = feature_importance_df.head(15).sort_values('Importancia', ascending=True)\n",
    "bars = ax.barh(top_features['Caracteristica'], top_features['Importancia'], color='#3498db')\n",
    "ax.set_xlabel('Importancia', fontsize=12)\n",
    "ax.set_title('Top 15 Caracteristicas Mas Importantes\\n(Random Forest)', fontsize=14)\n",
    "for bar, val in zip(bars, top_features['Importancia']):\n",
    "    ax.text(val + 0.002, bar.get_y() + bar.get_height()/2, '{:.3f}'.format(val), va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('grafico_6_importancia_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n   Grafico 6 guardado: grafico_6_importancia_features.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. GUARDADO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 13: GUARDAR MODELO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[12] GUARDADO DEL MODELO\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Seleccionar el mejor modelo basado en accuracy\n",
    "if acc_rf >= acc_dt:\n",
    "    mejor_modelo = rf_pipeline\n",
    "    mejor_nombre = \"Random Forest\"\n",
    "    mejor_acc = acc_rf\n",
    "else:\n",
    "    mejor_modelo = dt_pipeline\n",
    "    mejor_nombre = \"Arbol de Decision\"\n",
    "    mejor_acc = acc_dt\n",
    "\n",
    "print(\"\\n    Mejor modelo seleccionado: {} (Accuracy: {:.1f}%)\".format(mejor_nombre, mejor_acc*100))\n",
    "\n",
    "model_data = {\n",
    "    'model': mejor_modelo,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_columns': feature_columns,\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features\n",
    "}\n",
    "\n",
    "with open('modelo_rendimiento_academico.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"    Modelo guardado: modelo_rendimiento_academico.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XI. PRUEBA DE INFERENCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 14: PRUEBA DE INFERENCIA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[13] PRUEBA DE INFERENCIA\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Crear datos de prueba - Estudiante con alto uso de RRSS\n",
    "estudiante_prueba_1 = pd.DataFrame({\n",
    "    'Edad': [20],\n",
    "    'Ciclo': [5],\n",
    "    'Horas_Redes_Sociales': [7.0],\n",
    "    'Horas_Estudio': [1.5],\n",
    "    'Red_Social_Principal': ['TikTok'],\n",
    "    'Motivo_Uso': ['Entretenimiento'],\n",
    "    'Afecta_Concentracion': ['Siempre'],\n",
    "    'Afecta_Horas_Estudio': ['Si'],\n",
    "    'Usa_Estrategias': ['No'],\n",
    "    'Impacto_General': ['Negativo']\n",
    "})\n",
    "\n",
    "print(\"\\nCASO 1: Estudiante con alto uso de RRSS\")\n",
    "print(estudiante_prueba_1.to_string(index=False))\n",
    "\n",
    "prediccion_1 = mejor_modelo.predict(estudiante_prueba_1)[0]\n",
    "probabilidades_1 = mejor_modelo.predict_proba(estudiante_prueba_1)[0]\n",
    "\n",
    "print(\"\\nResultado de la prediccion:\")\n",
    "print(\"    Rendimiento predicho: {}\".format(label_encoder.classes_[prediccion_1]))\n",
    "print(\"\\nProbabilidades:\")\n",
    "for i, clase in enumerate(label_encoder.classes_):\n",
    "    print(\"    {}: {:.1f}%\".format(clase, probabilidades_1[i]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de prueba - Estudiante con bajo uso de RRSS\n",
    "estudiante_prueba_2 = pd.DataFrame({\n",
    "    'Edad': [22],\n",
    "    'Ciclo': [7],\n",
    "    'Horas_Redes_Sociales': [2.0],\n",
    "    'Horas_Estudio': [5.0],\n",
    "    'Red_Social_Principal': ['WhatsApp'],\n",
    "    'Motivo_Uso': ['Academico'],\n",
    "    'Afecta_Concentracion': ['Rara vez'],\n",
    "    'Afecta_Horas_Estudio': ['No'],\n",
    "    'Usa_Estrategias': ['Si'],\n",
    "    'Impacto_General': ['Positivo']\n",
    "})\n",
    "\n",
    "print(\"\\nCASO 2: Estudiante con bajo uso de RRSS y buenos habitos\")\n",
    "print(estudiante_prueba_2.to_string(index=False))\n",
    "\n",
    "prediccion_2 = mejor_modelo.predict(estudiante_prueba_2)[0]\n",
    "probabilidades_2 = mejor_modelo.predict_proba(estudiante_prueba_2)[0]\n",
    "\n",
    "print(\"\\nResultado de la prediccion:\")\n",
    "print(\"    Rendimiento predicho: {}\".format(label_encoder.classes_[prediccion_2]))\n",
    "print(\"\\nProbabilidades:\")\n",
    "for i, clase in enumerate(label_encoder.classes_):\n",
    "    print(\"    {}: {:.1f}%\".format(clase, probabilidades_2[i]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XII. RESUMEN FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECCION 15: RESUMEN FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN FINAL DEL PROYECTO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATASET:\")\n",
    "print(\"   - Total de registros: {}\".format(len(data)))\n",
    "print(\"   - Variables predictoras: {}\".format(len(feature_columns)))\n",
    "print(\"   - Variable objetivo: Rendimiento_Academico (Bajo, Promedio, Alto)\")\n",
    "\n",
    "print(\"\\n2. DISTRIBUCION DE CLASES:\")\n",
    "for clase in ['Alto', 'Promedio', 'Bajo']:\n",
    "    count = len(data[data['Rendimiento_Academico'] == clase])\n",
    "    print(\"   - {}: {} ({:.1f}%)\".format(clase, count, count/len(data)*100))\n",
    "\n",
    "print(\"\\n3. CORRELACIONES CLAVE:\")\n",
    "print(\"   - Horas_Redes_Sociales vs Promedio: r = {:.3f}\".format(\n",
    "    data['Horas_Redes_Sociales'].corr(data['Promedio_Ponderado'])))\n",
    "print(\"   - Horas_Estudio vs Promedio: r = {:.3f}\".format(\n",
    "    data['Horas_Estudio'].corr(data['Promedio_Ponderado'])))\n",
    "\n",
    "print(\"\\n4. RENDIMIENTO DEL MODELO:\")\n",
    "print(\"   - Arbol de Decision:\")\n",
    "print(\"     * CV: {:.1f}% (+/- {:.1f}%)\".format(dt_cv_scores.mean()*100, dt_cv_scores.std()*200))\n",
    "print(\"     * Test: {:.1f}%\".format(acc_dt*100))\n",
    "print(\"   - Random Forest:\")\n",
    "print(\"     * CV: {:.1f}% (+/- {:.1f}%)\".format(rf_cv_scores.mean()*100, rf_cv_scores.std()*200))\n",
    "print(\"     * Test: {:.1f}%\".format(acc_rf*100))\n",
    "\n",
    "print(\"\\n5. CARACTERISTICAS MAS IMPORTANTES:\")\n",
    "for idx, row in feature_importance_df.head(3).iterrows():\n",
    "    print(\"   - {}: {:.1f}%\".format(row['Caracteristica'], row['Importancia']*100))\n",
    "\n",
    "print(\"\\n6. ARCHIVOS GENERADOS:\")\n",
    "print(\"   - dataset_rrss_rendimiento_limpio.xlsx\")\n",
    "print(\"   - modelo_rendimiento_academico.pkl\")\n",
    "print(\"   - grafico_1_distribucion.png\")\n",
    "print(\"   - grafico_2_correlaciones.png\")\n",
    "print(\"   - grafico_3_analisis_categorico.png\")\n",
    "print(\"   - grafico_4_heatmap.png\")\n",
    "print(\"   - grafico_5_matrices_confusion.png\")\n",
    "print(\"   - grafico_6_importancia_features.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIN DEL ANALISIS - LISTO PARA DEPLOY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## XIII. CONCLUSIONES\n",
    "\n",
    "1. **Correlacion negativa significativa** (r = -0.68) entre horas de uso de redes sociales y rendimiento academico, confirmando la hipotesis inicial.\n",
    "\n",
    "2. **Horas de estudio** es el predictor mas fuerte del rendimiento academico, con una correlacion positiva de r = 0.74.\n",
    "\n",
    "3. El modelo **Random Forest** logro una exactitud realista y reproducible, sin indicios de overfitting.\n",
    "\n",
    "4. Los estudiantes que usan redes sociales con fines **academicos** presentan mejor rendimiento que aquellos que las usan principalmente para entretenimiento.\n",
    "\n",
    "5. El promedio de **4.5 horas diarias** en redes sociales coincide con la literatura internacional y estudios peruanos recientes.\n",
    "\n",
    "---\n",
    "\n",
    "**Siguiente paso:** Despliegue del sistema usando Streamlit (ver Seccion VII del informe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
